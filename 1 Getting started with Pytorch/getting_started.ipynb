{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Pytorch for Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There has been significant progress in computer vision because of deep learning in recent years. This helped to improve the performance of various tasks such as image recognition, object detection, image segmentation, and image generation. Deep learning frameworks and libraries have played a major role in this process. PyTorch, as a deep learning library, has emerged since 2016 and gained great attention among deep learning practitioners due to its flexibility and ease of use. \n",
    "\n",
    "There are several frameworks that practitioners use to build deep learning algorithms. In this book, we will use the latest version of PyTorch 1.0 to develop and train various deep learning models. PyTorch is a deep learning framework developed by Facebook's artificial intelligence research group. It provides flexibility and ease of use at the same. If you are familiar with other deep learning frameworks, you will find PyTorch very enjoyable.\n",
    "\n",
    "In this chapter, we will provide a review of deep learning concepts and their implementation using PyTorch 1.0. We will cover the following recipes:\n",
    "\n",
    "* Installing software tools and packages \n",
    "* Working with PyTorch tensors\n",
    "* Loading and processing data\n",
    "* Building models\n",
    "* Defining the loss function and optimizer\n",
    "* Training and evaluation\n",
    "\n",
    "Developing deep learning algorithms is comprised of two steps: training and deployment. In the training step, we use training data to train a model or network. In the deployment step, we deploy the trained model to predict the target values for new inputs.\n",
    "\n",
    "To train deep learning algorithms, the following ingredients are required:\n",
    "\n",
    "* Training data (inputs and targets) \n",
    "* The model (also called the network) \n",
    "* The loss function (also called the objective function or criterion)\n",
    "* The optimizer\n",
    "You can see the interaction between these elements in the following diagram:\n",
    "\n",
    "![Alt text](elements_interaction.png)\n",
    "\n",
    "The training process for deep learning algorithms is an iterative process. In each iteration, we select a batch of training data. Then, we feed the data to the model to get the model output. After that, we calculate the loss value. Next, we compute the gradients of the loss function with respect to the model parameters (also known as the weights). Finally, the optimizer updates the parameters based on the gradients. This loop continues. We also use a validation dataset to track the model's performance during training. We stop the training process when the performance plateaus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Verifying the installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12.0'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1050'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Working with Pytorch tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is built on tensors. A PyTorch tensor is an $n$-dimensional array, similar to NumPy arrays.\n",
    "\n",
    "If you are familiar with NumPy, you will see a similarity in the syntax when working with tensors, as shown in the following table:\n",
    "\n",
    "| Numpy Arrays          | Pytorch tensors           | Description                       |\n",
    "| --------------------- | ------------------------- | --------------------------------- |\n",
    "| `numpy.ones()`        | `torch.ones()`            | Create an array of ones           |\n",
    "| `numpy.zeros()`       | `torch.zeros()`           | Create an arry of zeros           |\n",
    "| `numpy.random.rand()` | `torch.rand()`            | Create a random array             |\n",
    "| `numpy.array()`       | `torch.tensor()`          | Create an array from given values |\n",
    "| `x.shape`             | `x.shape()` or `x.size()` | Get an array shape                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to do it**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Defining the tensor data type**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default tensor data type is `torch.float32`. This is the most used data type for tensor operations. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 1 Define a tensor with a default data type:\n",
    "x = torch.ones(2, 2)\n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int8)\n",
      "torch.int8\n"
     ]
    }
   ],
   "source": [
    "# 2 Specify the data type when defining a tensor:\n",
    "# Define a tensor with specific data type\n",
    "x = torch.ones(2, 3, dtype=torch.int8)\n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Changing the tensor's data type**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change a tensor's data type using the `.type` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# 1 Define a tensor with the torch.uint8 type\n",
    "x = torch.ones(1, dtype=torch.uint8)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 2 Change the tensor data type\n",
    "x = x.type(torch.float)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Converting tensors into numpy arrays** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily convert PyTorch tensors into numpy arrays. Let's take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5754, 0.1283, 0.7912],\n",
      "        [0.3042, 0.6715, 0.5209]])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 1 Define a tensor\n",
    "x = torch.rand(2, 3)\n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.575415   0.12833732 0.7912468 ]\n",
      " [0.30416185 0.67146844 0.52087986]]\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "# 2 Convert the tensor inot a numpy array\n",
    "y = x.numpy()\n",
    "print(y)\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Converting numpy arrays into tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also convert NumPy arrays into PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "# 1 Define a numpt array\n",
    "import numpy as np\n",
    "x = np.zeros((2, 2), dtype=np.float32)\n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 2 Convert the numpy array into a pytorch tensor\n",
    "y = torch.from_numpy(x)\n",
    "print(y)\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Moving tensors between devices**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, PyTorch tensors are stored on the CPU. PyTorch tensors can be utilized on a GPU to speed up computing. This is the main advantage of tensors compared to NumPy arrays. To get this advantage, we need to move the tensors to the CUDA device. We can move tensors onto any device using the `.to` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5000, 2.0000])\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# 1 Define a tensor on the CPU\n",
    "x = torch.tensor([1.5, 2])\n",
    "print(x)\n",
    "print(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5000, 2.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 2 Define a CUDA device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "\n",
    "# 3 Move the tensor onto the CUDA device\n",
    "x = x.to(device)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5000, 2.0000])\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# 4 Similarly, we can move tensors to CPU\n",
    "# define a cpu device\n",
    "device = torch.device(\"cpu\")\n",
    "x = x.to(device) \n",
    "print(x)\n",
    "print(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 5 We can also directly create a tensor on any device:\n",
    "# define a tensor on device\n",
    "device = torch.device(\"cuda:0\")\n",
    "x = torch.ones(2,2, device=device) \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How it works**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we defined a tensor, obtained the tensor type, and changed its type. Then, we converted PyTorch tensors into NumPy arrays and vice versa. We also moved tensors between the CPU and CUDA devices. Next, we showed you how to change a tensor data type using the `.type` method. Then, we showed how to convert PyTorch tensors into NumPy arrays using the `.numpy` method. \n",
    "\n",
    "After that, we showed you how to convert a NumPy array into a PyTorch tensor using the `.from_numpy(x)` method. Then, we showed you how to move tensors from a CPU device to a GPU device and vice versa, using the `.to` method. As you have seen, if you do not specify the device, the tensor will be hosted on the CPU device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading and processing data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases, it's assumed that we **receive data in three groups**: **training**, **validation**, and **test**. We use the **training dataset to train the model**. The **validation dataset is used to track the model's performance during training**. We use the **test dataset for the final evaluation of the model**. The target values of the test dataset are usually hidden from us. **We need at least one training dataset and one validation dataset to be able to develop and train a model**. Sometimes, we receive only one dataset. In such cases, we can split the dataset into two or three groups.\n",
    "\n",
    "Each dataset is comprised of inputs and targets. It is common to represent the inputs with `x` or `X` and the targets with `y` or `Y`. We add the suffixes `train`, `val`, and `test` to distinguish each dataset.\n",
    "\n",
    "In this recipe, we will learn about PyTorch data tools. We can use these tools to load and process data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to do it**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Loading the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PyTorch `torchvision` package provides multiple popular datasets. Let's load MNIST dataset from `torchvision`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "# 1 First, we will load the MNIST training dataset\n",
    "from torchvision import datasets\n",
    "# path to store the data and/or load from\n",
    "path2data = '../data'\n",
    "\n",
    "# loading training data\n",
    "train_data = datasets.MNIST(path2data, train=True, download=True)\n",
    "\n",
    "# 2 Then we will extract the input data and target labels\n",
    "# extract data and targets\n",
    "X_train, y_train = train_data.data, train_data.targets\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 28, 28])\n",
      "torch.Size([10000])\n",
      "torch.Size([60000, 1, 28, 28])\n",
      "torch.Size([10000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# 3 Next, we will load the MNIST test dataset:\n",
    "# loading validation data\n",
    "val_data = datasets.MNIST(path2data, train=False, download=True)\n",
    "\n",
    "# 4 Then, we will extract the input data and target labels:\n",
    "# extract data and targets\n",
    "X_val, y_val = val_data.data, val_data.targets\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "# 5 After that, we will add a new dimension to the tensors:\n",
    "# add a dimension to tensor to become B*C*H*W\n",
    "if len(X_train.shape) == 3:\n",
    "    X_train = X_train.unsqueeze(1)\n",
    "print(X_train.shape)\n",
    "\n",
    "if len(X_val.shape)==3:\n",
    "    X_val = X_val.unsqueeze(1)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's display a few sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 152, 242])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6 Next, we will import the required packages:\n",
    "from torchvision import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# 7 Then, we will define a helper function to display tensors as images:\n",
    "def show(img):\n",
    "    # convert tensor to numpy array\n",
    "    npimg = img.numpy()\n",
    "    # Convert to H*W*C shape\n",
    "    npimg_tr=np.transpose(npimg, (1,2,0))\n",
    "    plt.imshow(npimg_tr, interpolation='nearest')\n",
    "\n",
    "# 8 Next, we will create a grid of images and display them:\n",
    "# make a grid of 40 images, 8 images per row\n",
    "X_grid = utils.make_grid(X_train[:40], nrow=8, padding=2)\n",
    "print(X_grid.shape)\n",
    "\n",
    "# call helper function\n",
    "show(X_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data transformation** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image transformation (also called augmentation) is an effective technique that's used to improve a model's performance. The `torchvision` package provides common image transformations through the transform class. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'transformed')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1 Let's define a transform class in order to apply so some image transformations on the MNIST dataset\n",
    "from torchvision import transforms\n",
    "# loading MNIST training dataset\n",
    "train_data=datasets.MNIST(path2data, train=True, download=True)\n",
    "\n",
    "# define transformations\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=1),\n",
    "        transforms.RandomVerticalFlip(p=1),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "# 2 Let's apply the transformations on an image from the MNIST dataset:\n",
    "# get a sample image from training dataset\n",
    "img = train_data[0][0]\n",
    "\n",
    "# transform sample image\n",
    "img_tr=data_transform(img)\n",
    "\n",
    "# convert tensor to numpy array\n",
    "img_tr_np=img_tr.numpy()\n",
    "\n",
    "# show original and transformed images\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img,cmap=\"gray\")\n",
    "plt.title(\"original\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img_tr_np[0],cmap=\"gray\");\n",
    "plt.title(\"transformed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 We can also pass the transformer function to the dataset class:\n",
    "# define transformations\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(1),\n",
    "        transforms.RandomVerticalFlip(1),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "# Loading MNIST training data with on-the-fly transformations\n",
    "train_data=datasets.MNIST(path2data, train=True, download=True, transform=data_transform )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Wrapping tensors into a dataset** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your **data is available in tensors**, you can **wrap them as a PyTorch dataset** using the `TensorDataset` class. This will make it **easier to iterate over data during training**. Let's get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n"
     ]
    }
   ],
   "source": [
    "# 1 Let's create a PyTorch dataset by wrapping x_train and y_train :\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# wrap tensors into a dataset\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "val_ds = TensorDataset(X_val, y_val)\n",
    "\n",
    "for x, y in train_ds:\n",
    "    print(x.shape, y.item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating data loaders**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To **easily iterate over the data during training, we can create a data loader** using the `DataLoader` class, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 28, 28])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# 1 Let's create two data loaders for the training and validation datasets:\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# create a data loader from dataset\n",
    "train_dl = DataLoader(train_ds, batch_size=8)\n",
    "val_dl = DataLoader(val_ds, batch_size=8)\n",
    "\n",
    "# iterate over batches\n",
    "for xb,yb in train_dl:\n",
    "    print(xb.shape)\n",
    "    print(yb.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How it works**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we imported the `datasets` package from `torchvision`. This package contains several famous datasets, including **MNIST**. Then, we downloaded the **MNIST** training dataset into a local folder. Once downloaded, you can set the `download` flag to `False` in future runs. Next, we extracted the input data and target labels into PyTorch tensors and printed their size. Here, the training dataset contains $60,000$ inputs and targets. Then, we repeated the same step for the MNIST test dataset. To download the MNIST test dataset, we set the `train` flag to False. Here, the test dataset contains $10,000$ inputs and targets.\n",
    "\n",
    "Next, we added a new dimension to the input tensors since **we want the tensor shape to be `B*C*H*W`**, where `B`, `C`, `H`, and `W` are `batch size`, `channels`, `height`, and `width`, respectively. This is the **common shape for the inputs tensors in PyTorch**. Then, we defined a helper function to display sample images. We used `utils` from `torchvision` to create a grid of $40$ images in $5$ rows and $8$ columns.\n",
    "\n",
    "In the Data transformation subsection, we introduced the `torchvision.transforms` package. This package provides multiple transformation functions. We composed the `RandomHorizontalFlip` and `RandomVerticalFlip` methods to augment the dataset and the `ToTensor` method to convert images into PyTorch tensors. The **probability of horizontal and vertical flips** was set to $p=1$ to **enforce flipping in the next step**. We employed the data transformer on a sample image. Check out the original and the transformed image. The transformed image has been flipped both vertically and horizontally.\n",
    "\n",
    "Then, we passed the transformer function to the dataset class. This way, **data transformation will happen on-the-fly**. This is a **useful technique for large datasets that cannot be loaded into memory all at once**.\n",
    "\n",
    "In the *Wrapping tensors into a dataset* subsection, we created a dataset from tensors. For example, we can create a PyTorch dataset by wrapping x_train and y_train. This technique will be useful for **cases where the input and output data is available as tensors**.\n",
    "\n",
    "In the *Creating data loaders* subsection, we used the `DataLoader` class to define data loaders. This is a **good technique to easily iterate over datasets during training or evaluation**. When creating a data loader, **we need to specify the batch size**. We created two data loaders from `train_ds` and `val_ds`. Then, we extracted a mini-batch from `train_dl`. Check out the shape of the mini-batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Building models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **model** is **a collection of connected layers that process the inputs to generate the outputs**. You can use the `nn` package to define models. The `nn` package is a **collection of modules that provide common deep learning layers**. A module or layer of `nn` **receives input tensors, computes output tensors**, **and holds the weights**, if any. There are two methods we can use to define models in PyTorch: `nn.Sequential` and `nn.Module`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to do it**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a linear layer, a $2$-layer network, and a multilayer convolutional network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Defining a linear layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a linear layer and print out its output size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "# input tensor dimension 64*1000\n",
    "input_tensor = torch.randn(64, 1000)\n",
    "\n",
    "# linear layer with 1000 inputs and 100 output\n",
    "linear_layer = nn.Linear(1000, 100)\n",
    "\n",
    "# output of the linear layer\n",
    "output = linear_layer(input_tensor)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Defining models using `nn.Sequential`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `nn.Sequential` package to **create a deep learning model by passing layers in order**. Consider the $2$-layer neural network depicted in the following image:\n",
    "\n",
    "![](2_layer_net.png)\n",
    "\n",
    "As we can see, the network has $4$ nodes as input, $5$ nodes in the hidden layer, and $1$ node as the output. Next, we will show you how to implement the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=5, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 1 Let's implement and print the model using nn.Sequential\\\n",
    "from torch import nn\n",
    "\n",
    "# define a 2-layer model\n",
    "model = nn.Sequential(\n",
    "        nn.Linear(4, 5),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(5, 1),\n",
    "    )\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Defining models usings `nn.Module`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of defining models in PyTorch is by subclassing the `nn.Module`a class. In this method, we **specify the layers** in the `__init__` method of the class. Then, in the `forward` method, we **apply the layers to inputs**. This method **provides better flexibility for building customized models**.\n",
    "\n",
    "Consider a multilayer model, as shown in the following image:\n",
    "\n",
    "![](multilayerlayer_model.png)\n",
    "\n",
    "As seen in the preceding image, the model has two convolutional layers and two fully connected layers. Next, we will show you how to implement the model.\n",
    "\n",
    "Let's implement the multilayer model using `nn.Module`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
      "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 1 First, we implement the multilayer model using nn.Module\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    # 2 Then, we will define the __init__ function:\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    # 3 Next, we will define the forward function:\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "        # 4 Then, we will override both class functions, __init__ and forward:\n",
    "        # Net.__init__ = __init__\n",
    "        # Net.forward = forward\n",
    "\n",
    "# 5 Next, we will create an object of the Net class and print the model:\n",
    "model = Net()    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Moving the model to a CUDA devide**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model is a collection of parameters. By default, the model will be hosted on the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 3.7065e-03,  4.8027e-02,  8.1465e-02, -1.5327e-01,  1.9160e-01],\n",
      "          [ 7.6873e-02, -7.7875e-02,  1.8059e-01,  1.9553e-01, -1.6257e-01],\n",
      "          [-1.1791e-01, -1.5735e-02, -1.0542e-02,  1.8242e-01, -7.6568e-02],\n",
      "          [-1.9482e-02,  2.3838e-03,  1.8803e-01,  2.8744e-03, -8.0280e-02],\n",
      "          [-1.4653e-01,  7.2406e-02, -1.9928e-01, -1.7073e-01,  1.0188e-01]]],\n",
      "\n",
      "\n",
      "        [[[-9.5549e-02, -4.4911e-02, -1.2313e-01, -8.5943e-02,  9.2431e-02],\n",
      "          [-1.7169e-01, -1.5108e-01,  1.0783e-01, -1.8635e-01, -1.5265e-01],\n",
      "          [ 6.6771e-02, -9.1921e-02, -1.2419e-01,  1.0851e-01, -1.0471e-01],\n",
      "          [-1.8647e-01, -3.8100e-02, -4.3352e-02,  5.4372e-02, -2.6391e-02],\n",
      "          [-1.1983e-01, -9.1693e-02,  8.6923e-03,  2.0887e-02,  3.4373e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9464e-01, -4.2840e-02,  1.0328e-01, -4.2482e-02, -3.4480e-02],\n",
      "          [-4.0498e-02, -8.4061e-02, -1.0983e-01, -1.1285e-01,  2.1171e-02],\n",
      "          [-1.4118e-01, -6.7420e-02,  9.8690e-02, -1.9660e-01, -1.9838e-01],\n",
      "          [ 6.2569e-02, -5.5898e-02, -1.7945e-01,  1.7959e-01, -1.0869e-01],\n",
      "          [-9.3297e-02,  9.0653e-02,  1.9911e-01,  1.7265e-01, -3.4426e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 7.4400e-02,  9.3965e-02,  3.7188e-02,  1.1740e-01, -9.1759e-02],\n",
      "          [-5.9301e-02,  3.7627e-02, -3.4671e-02,  1.6604e-01,  9.7540e-02],\n",
      "          [-1.0970e-01,  1.5097e-01, -6.0416e-02, -1.7040e-01, -2.8697e-02],\n",
      "          [ 1.1083e-01, -7.9904e-02,  4.6471e-02, -1.9357e-02, -4.1315e-02],\n",
      "          [ 1.0781e-01,  1.2507e-01,  2.5333e-02,  1.7230e-01,  1.8021e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 3.1903e-02,  1.0563e-01, -1.2111e-01, -5.1734e-02,  7.2134e-02],\n",
      "          [-1.1384e-01, -1.5318e-01,  1.2088e-01, -1.1816e-01,  1.3170e-01],\n",
      "          [-2.4572e-02, -1.5603e-01, -1.2704e-01,  1.8925e-01, -1.1283e-01],\n",
      "          [-1.0449e-01,  2.8497e-02, -1.3836e-03,  1.8359e-01, -1.2618e-01],\n",
      "          [ 1.3420e-01, -1.3017e-01,  5.7586e-02, -1.6520e-01, -6.4807e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.2352e-02,  1.4426e-02, -1.7538e-01, -1.4664e-02, -3.8412e-02],\n",
      "          [ 6.6629e-02, -9.0055e-02,  1.5621e-01,  1.3756e-01, -1.8294e-01],\n",
      "          [ 3.9158e-02, -4.2974e-02, -4.6936e-03,  1.2167e-01, -8.9740e-02],\n",
      "          [-1.0619e-01, -4.3134e-02, -4.5531e-03, -1.3061e-01,  1.9906e-01],\n",
      "          [-8.3115e-02, -1.7826e-01,  1.6817e-01,  1.6995e-01,  6.1982e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 7.0665e-02, -1.2512e-01, -2.6307e-02,  5.7428e-03, -8.3105e-02],\n",
      "          [-4.4611e-02,  1.7018e-01, -9.1152e-02, -1.0969e-01,  1.0281e-01],\n",
      "          [ 8.8181e-02, -8.8349e-02, -1.2148e-01,  4.9348e-02,  1.5597e-01],\n",
      "          [ 8.3072e-02, -8.4848e-02, -9.5708e-02,  1.8425e-01, -2.5929e-02],\n",
      "          [-1.4322e-01,  1.4761e-01,  1.8036e-01, -4.5134e-02, -1.9456e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 8.9744e-02, -1.5387e-02,  8.1593e-02, -4.4255e-02,  8.4940e-02],\n",
      "          [ 2.7055e-02, -2.5145e-02, -8.0333e-02,  6.2572e-03, -1.1704e-01],\n",
      "          [-1.1725e-01,  1.7898e-01, -5.2190e-02,  7.9993e-02,  8.7907e-02],\n",
      "          [ 1.3908e-01,  1.0537e-01,  7.0032e-03,  3.2490e-02, -9.5897e-02],\n",
      "          [ 1.2659e-01,  1.4571e-02, -1.9195e-01,  1.5210e-01,  1.5384e-01]]],\n",
      "\n",
      "\n",
      "        [[[-4.0615e-02,  1.8237e-01, -1.3530e-01, -4.8005e-02, -6.3971e-02],\n",
      "          [ 1.7317e-01, -1.9155e-01, -7.4129e-03, -9.3415e-02,  1.3310e-01],\n",
      "          [-1.7865e-01,  1.1380e-01,  4.8741e-02,  4.7955e-02,  1.3386e-01],\n",
      "          [-3.7476e-02, -1.4284e-01,  1.6762e-01,  1.0630e-01,  5.9012e-03],\n",
      "          [-1.3687e-03, -8.5604e-02, -1.1768e-03, -1.2321e-01,  1.9332e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1245e-02, -1.1314e-01,  2.3432e-02,  1.0049e-02, -1.7344e-01],\n",
      "          [-1.7910e-01,  1.8231e-01, -9.7296e-02, -1.4726e-01,  1.6706e-01],\n",
      "          [ 1.8657e-01, -1.3015e-01, -2.7665e-02,  1.7309e-01,  3.8619e-02],\n",
      "          [-6.6518e-02, -1.7525e-01, -1.2065e-01, -1.6675e-01,  5.9867e-04],\n",
      "          [-1.3840e-01, -1.1433e-01, -1.1664e-01,  1.8575e-01,  1.5884e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.6307e-01, -1.4359e-01, -5.4039e-03,  5.1203e-02, -1.4024e-01],\n",
      "          [ 4.8911e-02,  8.2675e-02,  1.8240e-02, -1.1456e-01, -8.5298e-02],\n",
      "          [-1.5180e-01,  3.1611e-02, -1.4658e-02, -9.6860e-03, -1.7996e-01],\n",
      "          [ 1.0840e-01, -1.8371e-01,  1.2589e-03,  3.4009e-02,  9.5557e-02],\n",
      "          [-1.9058e-01, -2.4164e-04,  7.7429e-02, -8.1082e-03,  1.7198e-01]]],\n",
      "\n",
      "\n",
      "        [[[-5.5876e-02,  9.9205e-02, -1.2563e-01,  1.5151e-01,  3.4773e-02],\n",
      "          [-8.7844e-02,  1.7761e-01,  4.8251e-02,  1.6005e-01,  2.7349e-02],\n",
      "          [-1.2427e-01, -5.7310e-02, -1.9307e-01, -1.0488e-01,  1.4690e-01],\n",
      "          [-6.1991e-02,  7.8355e-02,  2.7214e-02, -1.4077e-02, -1.7598e-01],\n",
      "          [ 4.8756e-03,  1.3540e-01, -4.0009e-02,  1.7597e-02, -2.3372e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.8833e-01,  4.4324e-02,  9.9851e-02,  1.2621e-01,  5.0399e-02],\n",
      "          [-1.0772e-01, -3.6407e-02, -1.6470e-02,  1.2050e-01,  1.7129e-01],\n",
      "          [-7.0884e-03,  1.6826e-01, -3.9859e-02, -7.3268e-02,  9.7330e-02],\n",
      "          [-1.7712e-01,  1.4932e-01, -1.9663e-01, -1.7718e-01, -1.5296e-01],\n",
      "          [-7.3421e-03,  5.9066e-02,  4.3119e-02, -6.7066e-02,  5.1393e-02]]],\n",
      "\n",
      "\n",
      "        [[[-6.9364e-02, -3.7200e-02, -1.4060e-01, -1.9848e-01,  1.8204e-01],\n",
      "          [-1.3824e-01,  1.0173e-01, -9.7140e-02, -1.3794e-01, -1.6723e-01],\n",
      "          [-1.6774e-02, -4.1543e-02,  1.1934e-01,  5.1265e-02, -1.4364e-01],\n",
      "          [ 1.4120e-01,  1.3869e-01,  1.9713e-01,  1.2863e-01, -8.2742e-02],\n",
      "          [-9.4670e-02, -1.9882e-01, -1.9633e-01, -1.6661e-01,  2.8958e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.9273e-02, -1.6119e-01, -4.7665e-02,  6.1162e-02, -1.1816e-01],\n",
      "          [ 1.6466e-01,  3.2868e-02,  1.7711e-01, -2.5426e-02, -3.5556e-02],\n",
      "          [ 1.8682e-01, -4.8313e-02,  1.5209e-01, -1.8454e-01,  1.0004e-01],\n",
      "          [-1.0150e-01,  1.9811e-01,  1.1924e-01, -3.2706e-02,  9.1312e-02],\n",
      "          [-1.9689e-01,  7.8105e-02,  1.9773e-01,  8.5592e-03, -6.4946e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5159e-01, -6.7601e-02, -1.4250e-03, -1.9633e-01, -1.0477e-01],\n",
      "          [ 9.7250e-02, -1.1362e-01, -6.8190e-02, -1.3646e-01,  1.8867e-01],\n",
      "          [ 9.0596e-02, -1.8336e-01, -1.2009e-01,  8.0453e-03,  1.6770e-01],\n",
      "          [-4.8366e-03,  1.6727e-01,  1.3286e-01, -1.0406e-01,  3.9371e-02],\n",
      "          [-1.7420e-01, -1.2527e-01,  1.0358e-01,  6.8152e-02,  3.6739e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 9.5935e-02, -7.4071e-02, -4.7272e-02,  7.6074e-02, -1.6838e-01],\n",
      "          [-1.7199e-01, -1.5523e-01, -9.0356e-02,  1.9365e-01,  4.2663e-02],\n",
      "          [ 1.8019e-01, -5.1449e-02,  9.3478e-02,  5.7026e-03,  1.8709e-01],\n",
      "          [ 1.6941e-01, -1.2765e-01, -1.6774e-01,  2.1224e-02,  1.2448e-01],\n",
      "          [ 1.3265e-01, -1.8032e-01, -5.6306e-02, -1.0568e-02,  1.3398e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4315e-01,  1.5759e-01, -9.1871e-02, -1.7923e-01, -2.4145e-03],\n",
      "          [ 2.4156e-03, -1.3794e-01,  5.3970e-02, -6.5795e-02, -8.4287e-02],\n",
      "          [ 1.3270e-01, -5.0604e-02,  1.6097e-01, -1.7878e-01,  1.9650e-01],\n",
      "          [ 1.8754e-01,  1.8655e-01, -2.2181e-02, -5.3274e-02, -1.9888e-01],\n",
      "          [-2.9615e-02, -2.5732e-02, -1.9461e-01, -8.0171e-02,  3.9129e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3744e-01,  7.0901e-02, -8.5659e-02, -1.2571e-01,  9.3697e-02],\n",
      "          [-7.8401e-04,  1.9305e-01, -1.7513e-01, -1.9174e-01,  1.4511e-01],\n",
      "          [-1.9576e-01,  1.9566e-01, -5.9823e-02,  1.1349e-02, -3.3700e-02],\n",
      "          [-1.5823e-01, -8.1510e-02,  1.2688e-01, -2.0882e-02,  1.7678e-01],\n",
      "          [ 1.6718e-01, -1.3799e-01,  5.3796e-02,  1.5098e-01,  1.6477e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.2940e-02,  1.0154e-02,  1.9253e-01, -1.3477e-01,  1.2950e-02],\n",
      "          [ 1.6205e-01,  6.4841e-02,  8.4428e-02, -9.7892e-02, -1.1955e-01],\n",
      "          [ 7.8281e-02, -2.4857e-02, -1.3916e-01, -2.4027e-02,  4.1215e-02],\n",
      "          [ 1.6323e-01,  5.9159e-02,  3.8858e-02,  3.9164e-02, -1.3977e-02],\n",
      "          [-3.5886e-02, -1.9504e-01,  5.9027e-02, -1.9660e-01, -1.2723e-04]]]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 1 Let's get the model device\n",
    "print(next(model.parameters()).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 2 Then, we will move the model to the CUDA device:\n",
    "device = torch.device(\"cuda:0\")\n",
    "model.to(device)\n",
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Printing the model summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is usually helpful to get a summary of the model to see the output shape and the number of parameters in each layer. Printing a model does not provide this kind of information. We can use the torchsummary package from the following GitHub repository for this purpose [https://github.com/sksq96/pytorch-summary](https://github.com/sksq96/pytorch-summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummaryNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "# 1 Install the torchsummary package\n",
    "%pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 20, 24, 24]             520\n",
      "            Conv2d-2             [-1, 50, 8, 8]          25,050\n",
      "            Linear-3                  [-1, 500]         400,500\n",
      "            Linear-4                   [-1, 10]           5,010\n",
      "================================================================\n",
      "Total params: 431,080\n",
      "Trainable params: 431,080\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.12\n",
      "Params size (MB): 1.64\n",
      "Estimated Total Size (MB): 1.76\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 2 Let's get the model summary using torchsummary\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How it works**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we showed you how to create a linear layer using the `nn` package. The linear layer receives the input of the $64 \\times 1000$ dimension, holds the weights of the $1000 \\times 100$ dimension, and computes the output of the $64 \\times 100$ dimension.\n",
    "\n",
    "Next, we defined a $2$-layer neural network using `nn.Sequential`. There were four neurons in the input layer, $5$ neurons in the hidden layer, and $1$ neuron in the output layer. Using the `print` command, you can visualize the model's layers.\n",
    "\n",
    "Next, we defined a multilayer model using `nn.Module`. The model has two `Conv2d` layers and $2$ fully connected linear layers. For better code readability, we presented the `Net` class in a few snippets. First, we defined the bulk of the class. Then, we defined the `__init__` function. As you saw, $2$ `Conv2d` layers and $2$ linear layers were defined in this function. Next, we defined the `forward` function. In this function, we defined the outline of the model and the way layers are connected to each other.\n",
    "\n",
    "We used `relu` and `max_pool2d` from `torch.nn.functional` to define the activation function and pooling layers, respectively. Check out the way we used the `.view` method to flatten the extracted features from the `Conv2d` layers. The feature size was $4 \\times 4$ and there were $50$ channels in the `self.conv2` layer. Due to this, the flatten size is $50 \\times 4 \\times 4$. Also, check out the returned values from the `forward` function. As we saw, the `log_softmax` function was applied to the outputs. Next, we overrode the `Net` class functions. Finally, we created an object of the `Net` class and called it `model`. Then, we printed the model. Note that the `print` command does not show functional layers such as `relu` and `max_pool2d`.\n",
    "\n",
    "In the Moving the model to a CUDA device subsection, we verified that the model was hosted on the CPU device. Then, we moved the model to the CUDA device using the `.to` method. Here, we moved the first GPU or `\"cuda:0\"`. If your system is equipped with multiple GPU devices, you can select a different number, for instance, `\"cuda:2\"`.\n",
    "\n",
    "Next, we installed the `torchsummary` package in the conda environment using the provided command. If you do not want to install this package, the other option is to copy `torchsummary.py` into the folder of your code.\n",
    "\n",
    "To get a model summary using `torchsummary`, we need to pass the input dimension to the `summary` function. For our MNIST example, we passed $(1,28,28)$ as the input dimension and displayed the model summary. As seen, the output shape and the number of parameters of each layer, except functional layers, is shown in the summary. \n",
    "\n",
    "Finally, we got the model summary using the `torchsummary` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Defining the loss function and optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function computes the distance between the model outputs and targets. It is also called the objective function, cost function, or criterion. Depending on the problem, we will define the appropriate loss function. For instance, for classification problems, we usually define the cross-entropy loss. \n",
    "\n",
    "We use the optimizer to update the model parameters (also called weights) during training. The optim package in PyTorch provides implementations of various optimization algorithms. These include **stochastic gradient descent (SGD)** and its variants, that is, Adam, RMSprop, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to dot it**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Defining the loss function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a loss function and test it on a mini-batch. Let's get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.3324203491211\n"
     ]
    }
   ],
   "source": [
    "# 1 First, we will define the negative log-likelihood loss\n",
    "from torch import nn\n",
    "loss_func = nn.NLLLoss(reduction='sum')\n",
    "\n",
    "# 2 Let's test the loss function on a mini-batch\n",
    "for xb, yb in train_dl:\n",
    "    # move batch to cuda device\n",
    "    xb = xb.type(torch.float).to(device)\n",
    "    yb = yb.to(device)\n",
    "    # get model output\n",
    "    out = model(xb)\n",
    "    # calculate loss value\n",
    "    loss = loss_func(out, yb)\n",
    "    print(loss.item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Let's compute the gradients with respect to the model parameters:\n",
    "# compute gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define the optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the optimizer and present the steps backward. Let's get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Let's define the Adam optimizer\n",
    "from torch import optim\n",
    "opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 2 Use the following code to update the mode parameters\n",
    "# update model parameters\n",
    "opt.step()\n",
    "\n",
    "# 3 Next, we set the gradients to 0\n",
    "# set gradient to zero\n",
    "opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How it works**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we defined the loss function. We used the `torch.nn` package to define the ***negative log-likelihood loss***. This loss is **useful for training a classification problem with multiple classes**. The **input to this loss function should be log-probabilities**. If you recall from the Building models section, **we applied `log_softmax` at the output layer to get log-probabilities from the model**. Next, we presented the forward path. We **extracted a mini-batch**, **fed it to the model**, and **calculated the loss value**. Next, we used the `.backward` method to **compute the gradients of the loss with respect to the model parameters**. **This step will be used during the backpropagation algorithm**.\n",
    "\n",
    "Next, we define the `Adam` optimizer. The inputs to the optimizer are the model parameters and the learning rate. Then, we presented the `.step()` model to automatically update the model parameters. Don't forget to set the gradients to zero before computing the gradients of the next batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **See also**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torch.nn` package provides several common loss functions. For a list of supported loss functions, please visit the following link: https://pytorch.org/docs/stable/nn.html.\n",
    "\n",
    "For more information on the `torch.optim` package, please visit the following link: https://pytorch.org/docs/stable/optim.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training and evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the ingredients are ready, we can start training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to do it**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will develop helper functions for batch and epoch processing and training the model. Let's get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 0.045384, val loss: 0.050464, accuracy: 98.43\n",
      "epoch: 1, train loss: 0.026047, val loss: 0.041134, accuracy: 98.78\n",
      "epoch: 2, train loss: 0.018725, val loss: 0.048098, accuracy: 98.65\n",
      "epoch: 3, train loss: 0.012559, val loss: 0.044119, accuracy: 98.97\n",
      "epoch: 4, train loss: 0.011040, val loss: 0.044365, accuracy: 98.93\n"
     ]
    }
   ],
   "source": [
    "# 1 Let's develop a helper function to compute the loss value per mini-batch:\n",
    "def loss_batch(loss_func, xb, yb, yb_h, opt=None):\n",
    "    # obtain loss\n",
    "    loss = loss_func(yb_h, yb)\n",
    "\n",
    "    # obtain performance metric\n",
    "    metric_b = metrics_batch(yb, yb_h)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    return loss.item(), metric_b\n",
    "\n",
    "# 2 Next, we will define a helper function to compute the loss\n",
    "# and metric values for a dataset\n",
    "def metrics_batch(target, output):\n",
    "    # obtain output class\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "    # compare output class with target class\n",
    "    corrects = pred.eq(target.view_as(pred)).sum().item()\n",
    "    return corrects\n",
    "\n",
    "# 3 Next, we will define a helper function to compute the loss\n",
    "# and metric values for a dataset\n",
    "def loss_epoch(model, loss_func, dataset_dl, opt=None):\n",
    "    loss = 0.0\n",
    "    metric = 0.0\n",
    "    len_data = len(dataset_dl.dataset)\n",
    "    for xb, yb in dataset_dl:\n",
    "        xb = xb.type(torch.float).to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        # obtain model output\n",
    "        yb_h = model(xb)\n",
    "\n",
    "        loss_b, metric_b = loss_batch(loss_func, xb, yb, yb_h, opt)\n",
    "        loss+=loss_b\n",
    "        if metric_b is not None:\n",
    "            metric+=metric_b\n",
    "    loss /= len_data\n",
    "    metric /= len_data\n",
    "    return loss, metric\n",
    "\n",
    "# 4 Finally, we will define the train_val function\n",
    "def train_val(epochs, model, loss_func, opt, train_dl, val_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_metric=loss_epoch(model,loss_func,train_dl,opt)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_metric=loss_epoch(model,loss_func,val_dl)\n",
    "\n",
    "        accuracy = 100*val_metric\n",
    "\n",
    "        print(\"epoch: %d, train loss: %.6f, val loss: %.6f, accuracy: %.2f\" %(epoch, train_loss, val_loss, accuracy))\n",
    "\n",
    "# 5 Let's train the model for a few epochs:\n",
    "# call train_val function\n",
    "num_epochs=5\n",
    "train_val(num_epochs, model, loss_func, opt, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Storing and loading models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is complete, we'll want to store the trained parameters in a file for deployment and future use. There are two ways of doing so.\n",
    "\n",
    "* **Let's look at the first method:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 First, we will store the model parameter or state_dict in a file\n",
    "# define path2weights\n",
    "path2weights = \"../models/weights.pt\"\n",
    "# store state_dict to file\n",
    "torch.save(model.state_dict(), path2weights)\n",
    "\n",
    "# 2 To load the model parameters from the file, we will define an object of the Net class\n",
    "# define model: weights are randomly initialized\n",
    "_model = Net()\n",
    "\n",
    "# 3 Then, we will load state_dict from the file\n",
    "weights = torch.load(path2weights)\n",
    "\n",
    "# Next, we will set state_dict to the model\n",
    "_model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Now, let's look at the second method:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 First, we will store the model in a file:\n",
    "# define a path2model\n",
    "path2model=\"../models/model.pt\"\n",
    "\n",
    "# store model and weights into a file\n",
    "torch.save(model, path2model)\n",
    "\n",
    "# 2 To load the model parameters from the file, we will define an object of the Net class:\n",
    "# define model: weights are randomly initiated\n",
    "_model = Net()\n",
    "# Then, we will load the model from the local file:\n",
    "_model=torch.load(path2model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Deploying the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy a model, we need to load the model using the methods described in the previous section. Once the model has been loaded into memory, we can pass new data to the model. Let's get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1 To deploy the model on a sample image from the validation dataset, we will get a sample tensor\n",
    "n=100\n",
    "x= X_val[n]\n",
    "y=y_val[n]\n",
    "print(x.shape)\n",
    "plt.imshow(x.numpy()[0], cmap=\"gray\")\n",
    "\n",
    "torch.Size([1, 28, 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 6\n"
     ]
    }
   ],
   "source": [
    "# 2 Then, we will preprocess the tensor:\n",
    "# we use unsqueeze to expand dimensions to 1*C*H*W\n",
    "x= x.unsqueeze(0)\n",
    "\n",
    "# convert to torch.float32\n",
    "x=x.type(torch.float)\n",
    "\n",
    "# move to cuda device\n",
    "x=x.to(device)\n",
    "\n",
    "# 3 Next, we will get the model prediction:\n",
    "# get model output\n",
    "output=_model(x)\n",
    "\n",
    "# get predicted class\n",
    "pred = output.argmax(dim=1, keepdim=True)\n",
    "print (pred.item(),y.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How it works**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we developed a helper function to compute the **loss and metric value per mini-batch**. The `opt` argument of the function **refers to the optimizer**. **If given, the gradients are computed and the model parameters are updated per mini-batch**.\n",
    "\n",
    "Next, we developed a helper function to compute a **performance metric**. The performance metric can be defined depending on the task. Here, we chose the accuracy metric for our classification task. We used `output.argmax` to get **the predicted class with the highest probability**.\n",
    "\n",
    "Next, we defined a helper function to compute the loss and metric values for an entire dataset. We used the data loader object to get mini-batches, feed them to the model, and compute the loss and metrics per mini-batch. We used two running variables to add loss and metric values.\n",
    "\n",
    "Next, we defined a helper function to train the model for multiple epochs. In each epoch, we also evaluated the model's performance using the validation dataset. Note that we set the model in training and evaluation modes using `model.train()` and `model.eval()`, respectively. Moreover, we used `torch.no_grad()` to stop `autograd` from calculating the gradients during evaluation.\n",
    "\n",
    "Next, we explored two methods of storing the trained model. In the first method, we stored `state_dict` or model parameters only. Whenever we need the trained model for deployment, we have to create an object of the model, then load the parameters from the file, and then set the parameters to the model. This is the recommended method by PyTorch creators.\n",
    "\n",
    "In the second method, we stored the model into a file. In other words, we stored both the model and `state_dict` into one file. Whenever we need the trained model for deployment, we need to create an object of the `Net` class. Then, we loaded the model from the file. So, there is no actual benefit of doing this compared to the previous method.\n",
    "\n",
    "Next, we deployed the model on a sample image of the validation dataset. The sample image shape is `C x H x W`. Thus. we added a new dimension to become `1 x C x H xW`. Then, we converted the tensor type into `torch.float32` and moved it to a CUDA device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Make sure that the model and data are hosted on the same device at deployment, otherwise, you will encounter an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **There's more**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training deep learning models requires developing intuitions. We will introduce other techniques such as early stopping and learning rate schedules to avoid overfitting and improve performance in the next chapter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "580318c0c86e0542a7f2f9882bbbc393e6c88c07c2a75daeff3d2ea36de686a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
